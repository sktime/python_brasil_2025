{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelos de Machine Learning\n",
        "\n",
        "Nesse capítulo vamos ver as maneiras de usar modelos de Machine Learning para\n",
        "forecasting. Aqui é onde mais acontecem erros de novos praticantes, pois\n",
        "muitas vezes tentam aplicar modelos de ML diretamente na série temporal.\n",
        "\n",
        "\n",
        "Para usar um modelo de ML, precisamos transformar a série temporal em um problema de regressão tradicional. Isso é feito criando janelas deslizantes (sliding windows) da série temporal, onde cada janela é usada como uma amostra de treinamento para o modelo de ML.\n",
        "\n",
        "Ou seja, se temos uma série temporal $(y_t)$, podemos criar janelas de tamanho $n$ e usar os valores $y_{t-n}, y_{t-n+1}, \\ldots, y_{t-1}$ como características (features) para prever o valor $y_t$.\n",
        "\n",
        "![img/reduction.png](img/reduction.png)\n",
        "\n",
        "\n",
        "Para prever mais de um passo à frente, existem duas abordagens:\n",
        "\n",
        "1. **Previsão recursive**: se queremos prever $h$ passos à frente, podemos usar o modelo para prever $y_{t+1}$, depois usar essa previsão para prever $y_{t+2}$, e assim por diante, até $y_{t+h}$. Isso pode levar a erros acumulados, pois cada previsão depende das previsões anteriores.\n",
        "2. **Previsão direta**: em vez de prever um passo de cada vez, podemos treinar o modelo para prever todos os $h$ passos à frente de uma vez. Isso pode ser feito usando um modelo para cada $h$ ou usando um modelo que prevê um vetor de $h$ valores.\n",
        "\n",
        "A verdade é que as duas abordagens podem ser vistas como uma: a previsão recursiva pode ser vista como uma previsão direta para $h=1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: false\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "from tsbook.datasets.retail import SyntheticRetail\n",
        "from sktime.utils.plotting import plot_series\n",
        "from sktime.forecasting.naive import NaiveForecaster\n",
        "\n",
        "dataset = SyntheticRetail(\"univariate\")\n",
        "y_train, X_train, y_test, X_test = dataset.load(\n",
        "    \"y_train\", \"X_train\", \"y_test\", \"X_test\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## O problema da tendência\n",
        "\n",
        "A tendência em séries temporais é como um constante problema de data drift:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "_X = [y_train.iloc[i : i + 7] for i in range(0, 700)]\n",
        "\n",
        "_X_test = [y_train.iloc[i : i + 7] for i in range(700, 800)]\n",
        "\n",
        "\n",
        "def set_index(x):\n",
        "    x.index = range(len(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "_X = [set_index(x) for x in _X]\n",
        "_X_test = [set_index(x) for x in _X_test]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "for x in _X:\n",
        "    ax.plot(x, color=\"gray\", alpha=0.3)\n",
        "for x in _X_test:\n",
        "    ax.plot(x, color=\"red\", alpha=0.3)\n",
        "\n",
        "# Add legend, with 1 red line for test and 1 gray for train\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "legend_handles = [\n",
        "    Line2D([0], [0], color=\"gray\", alpha=0.3, lw=2, label=\"Treino\"),\n",
        "    Line2D([0], [0], color=\"red\", alpha=0.3, lw=2, label=\"Teste\"),\n",
        "]\n",
        "ax.legend(handles=legend_handles, loc=\"best\")\n",
        "ax.set_title(\"Série original - Magnitudes diferentes para cada janela\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quando criamos nossas janelas e olhamos treine e teste, esse problema fica claro. A informação de uma série em treino não é util para prever a série de teste, pois elas estão em magnitudes diferentes.\n",
        "\n",
        "Uma possível solução para isso é normalizar cada janela, dividindo pelo valor médio da janela. Assim, todas as janelas ficam na mesma escala:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "\n",
        "_X = [x / x.mean() for x in _X]\n",
        "_X_test = [x / x.mean() for x in _X_test]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "for x in _X:\n",
        "    ax.plot(x, color=\"gray\", alpha=0.3)\n",
        "for x in _X_test:\n",
        "    ax.plot(x, color=\"red\", alpha=0.3)\n",
        "\n",
        "ax.legend(handles=legend_handles, loc=\"best\")\n",
        "ax.set_title(\"Série normalizada\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "e podemos prever sem problemas.\n",
        "\n",
        "Outra possibilidade é a diferenciação, como já vimos em capítulos anteriores. A diferenciação remove a tendência da série, tornando-a estacionária.\n",
        "\n",
        "## Usando modelos de ML com sktime\n",
        "\n",
        "Primeiro, vamos import `ReductionForecaster`, que é a classe que implementa a abordagem de janelas deslizantes para usar modelos de ML em séries temporais. Vamos testar um primeiro caso sem nenhum tipo de preprocessamento, apenas criando as janelas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tsbook.forecasting.reduction import ReductionForecaster\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = ReductionForecaster(\n",
        "    RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    window_length=30,\n",
        ")\n",
        "\n",
        "model.fit(y_train, X=X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_pred = model.predict(fh=y_test.index, X=X_test)\n",
        "plot_series(y_train, y_test, y_pred, labels=[\"Treino\", \"Teste\", \"Previsão com ML\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Claramente, tivemos o problema que mencionamos anteriormente.\n",
        "\n",
        "\n",
        "### Solução 1: Diferenciação\n",
        "\n",
        "Uma solução é usar a diferenciação para remover a tendência da série. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sktime.transformations.series.difference import Differencer\n",
        "\n",
        "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "model = Differencer() * ReductionForecaster(\n",
        "    regressor,\n",
        "    window_length=30,\n",
        "    steps_ahead=1,\n",
        ")\n",
        "\n",
        "model.fit(y_train, X=X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_pred_diff = model.predict(fh=y_test.index, X=X_test)\n",
        "\n",
        "plot_series(\n",
        "    y_train, y_test, y_pred_diff, labels=[\"Treino\", \"Teste\", \"Previsão com ML + Diferença\"]\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aqui, já vemos uma melhora significativa. Mas tem algo que podemos melhorar para realizar a diferenciação? Sim. Lembre que essa série tem um padrão multiplicativo. Então, antes de aplicar a diferenciação, podemos aplicar uma transformação logarítmica para estabilizar a variância:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sktime.transformations.series.boxcox import LogTransformer\n",
        "\n",
        "model_log = LogTransformer() * model\n",
        "\n",
        "model_log.fit(y_train, X=X_train)\n",
        "y_pred_log_diff = model_log.predict(fh=y_test.index, X=X_test)\n",
        "plot_series(\n",
        "    y_train,\n",
        "    y_test,\n",
        "    y_pred_log_diff,\n",
        "    labels=[\"Treino\", \"Teste\", \"Previsão com ML + Log + Diferença\"],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solução 2: Normalização por janela\n",
        "\n",
        "A diferenciação aumenta o ruído da série, o que pode dificultar o trabalho do modelo de ML. Outra opção é normalizar em cada janela. A classe `ReductionForecaster` tem um parâmetro chamado `normalization_strategy`, que pode ser usado para determinar a estratégia de normalização. Vamos usar a estratégia `divide_mean`, que divide cada janela pelo seu valor médio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = ReductionForecaster(\n",
        "    regressor,\n",
        "    window_length=30,\n",
        "    steps_ahead=1,\n",
        "    normalization_strategy=\"divide_mean\",\n",
        ")\n",
        "\n",
        "model.fit(y_train, X=X_train)\n",
        "y_pred_norm = model.predict(fh=y_test.index, X=X_test)\n",
        "\n",
        "plot_series(\n",
        "    y_train, y_test, y_pred_norm, labels=[\"Treino\", \"Teste\", \"Previsão com ML + Normalização\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo direto e recursivo\n",
        "\n",
        "Podemos fazer um conjunto de modelos, um para cada passo à frente. Abaixo, definimos `steps_ahead=12`, o que significa que o modelo vai prever 12 passos à frente diretamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = ReductionForecaster(\n",
        "    regressor,\n",
        "    window_length=30,\n",
        "    steps_ahead=12,\n",
        "    normalization_strategy=\"divide_mean\",\n",
        ")\n",
        "\n",
        "model.fit(y_train, X=X_train)\n",
        "y_pred_norm_direct = model.predict(fh=y_test.index, X=X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_series(\n",
        "    y_train,\n",
        "    y_test,\n",
        "    y_pred_norm_direct,\n",
        "    labels=[\"Treino\", \"Teste\", \"Previsão com ML + Normalização\"],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos comparar o MAPE de todos os modelos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\n",
        "\n",
        "mape = MeanAbsolutePercentageError()\n",
        "\n",
        "results = {}\n",
        "for _y_pred, label in zip(\n",
        "    [\n",
        "        y_pred,\n",
        "        y_pred_diff,\n",
        "        y_pred_log_diff,\n",
        "        y_pred_norm,\n",
        "        y_pred_norm_direct,\n",
        "    ],\n",
        "    [\n",
        "        \"ML\",\n",
        "        \"ML + Diferença\",\n",
        "        \"ML + Log + Diferença\",\n",
        "        \"ML + Normalização\",\n",
        "        \"ML + Normalização + Direto\",\n",
        "    ],\n",
        "):\n",
        "    results[label] = mape(y_test, _y_pred)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "results = pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"MAPE\"])\n",
        "results.sort_values(\"MAPE\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/felipeangelim/Workspace/python_brasil_2025/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}