---
title: 'Sktime workshop: Pycon Colombia 2025 (Part 2)'
jupyter: python3
---


![](imgs/sktime-logo.png)

2. **Forecasting panel data with sktime** (30 min)
   1. Data representation for panel data
   2. Upcasting feature in sktime
   3. Probabilistic forecasting
   4. Panel forecasting with Machine Learning models


## 2.1. Loading the data


```{python}
import warnings
import pandas as pd
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore")
```

```{python}
from pycon_workshop.dataset import PyConWorkshopDataset

dataset = PyConWorkshopDataset("panel")

y_train, y_test, X_train, X_test = dataset.load("y_train", "y_test", "X_train", "X_test")

display(y_train)
```

```{python}
display(X_train)
```

```{python}
from sktime.utils.plotting import plot_series

fig, ax = plt.subplots(figsize=(10, 4))
y_train.unstack(level=0).droplevel(0, axis=1).iloc[:, :10].plot(ax=ax, alpha=0.4)
ax.legend([])
plt.show()
```

```{python}
from sktime.utils.plotting import plot_series

fig, ax = plt.subplots(figsize=(10, 4))
y_train.unstack(level=0).droplevel(0, axis=1).iloc[:, [0,10]].plot(ax=ax, alpha=0.7)
plt.show()
```

#### 2.1.1. Pandas for multiindex data

To work with such data structures, it is important to revisit some pandas operations.

```{python}
y_train.index.get_level_values(-1)
```

In pandas, the following operations are useful:

```{python}
y_train.index
```

```{python}
y_train.index.get_level_values(0).unique()
```

```{python}
y_train.loc[0]
```

```{python}
y_train.loc[pd.IndexSlice[[0,2], :]]
```

```{python}
fh = y_test.index.get_level_values(1).unique()
```

```{python}
fh
```

## 2.2. Automatic upcasting

Have you ever dreamed of a world that you do not need to change code to switch between univariate and panel data?

**No extra lines needed!** Automatically upcast to panel data when using `sktime` estimators.

```{python}
from sktime.forecasting.naive import NaiveForecaster


naive_forecaster = NaiveForecaster(strategy="last", window_length=1)
naive_forecaster.fit(y_train)
y_pred_naive = naive_forecaster.predict(fh=fh)

y_pred_naive
```

* Internally, sktime creates one clone of the estimator for each series in the panel data
* Then it fits each clone to the corresponding series.

```{python}
naive_forecaster.forecasters_.head()
```

**This is extremely useful for clean code and rapid prototyping!**


#### Metrics

Now that we have multiple series, we need to explain to the metric how to handle this!

* Use `multilevel="uniform_average_time"` to average the time series across the panel.
* Use `multilevel="raw_values"` to obtain the error per series.

```{python}
from sktime.performance_metrics.forecasting import MeanSquaredScaledError

metric = MeanSquaredScaledError(multilevel="uniform_average_time")
```

```{python}
metric(y_true=y_test, y_pred=y_pred_naive, y_train=y_train)
```

## 2.3. Machine learning models for timeseries forecasting

* We can apply ML Regressors to time series forecasting.
* We call this process **reduction**
 
![](imgs/global_reduction.png)

The `WindowSummarizer` creates the set of temporal tabular features for the ML model.

```{python}
from sktime.transformations.series.summarize import WindowSummarizer

summarizer = WindowSummarizer(
    lag_feature={
        "lag" : list(range(1,20)),
        "std" : [list(range(1,20))],
    },
)

summarizer.fit_transform(y_train, X_train)
```

* How to compute forecasts for multiple steps ahead?
* We can use two approaches:
  * `RecursiveTabularRegressionForecaster`: recursively predicts the next value and uses it as input for the next prediction.
  * `DirectTabularRegressionForecaster`: creates a separate model for each step ahead.

```{python}
from sktime.forecasting.compose import RecursiveTabularRegressionForecaster
from sklearn.ensemble import RandomForestRegressor

global_forecaster1 = RecursiveTabularRegressionForecaster(
    RandomForestRegressor(n_estimators=20, random_state=42),
    pooling="global",
    window_length=None,
    transformers=[summarizer]
)

global_forecaster1.fit(y_train, X_train)
```

```{python}
global_forecaster1.get_params()
```

```{python}
y_pred_global1 = global_forecaster1.predict(fh=fh, X=X_test)
```

```{python}
fig, ax = plt.subplots(figsize=(10, 4))
y_train.loc[10, "sales"].plot(ax=ax, label="Train")
y_test.loc[10, "sales"].plot(ax=ax, label="Test")
y_pred_global1.loc[10, "sales"].plot(ax=ax, label="Global 1")
plt.legend()
plt.show()
```

### Feature engineering is important!

* We should not think that ML models learn everything by themselves.
* We have to think as they think. They see values, not time series.
* The **scale becomes a feature** that allows the model to identify which series is it forecasting.
* We can standardize the different series to make them comparable.

```{python}
metric(y_true=y_test, y_pred=y_pred_global1, y_train=y_train)
```

```{python}
from sklearn.preprocessing import StandardScaler


global_forecaster2 = StandardScaler() * global_forecaster1

global_forecaster2.fit(y_train, X_train)
```

```{python}
y_pred_global2 = global_forecaster2.predict(fh=fh, X=X_test)
```

```{python}
metric(y_true=y_test, y_pred=y_pred_global2, y_train=y_train)
```

```{python}
fig, ax = plt.subplots(figsize=(10, 4))
y_train.loc[0].plot(ax=ax, label="Train")
y_test.loc[0].plot(ax=ax, label="Test")
y_pred_global2.loc[0].plot(ax=ax, label="Global 1")
```

* Only scaling each timeseries allows the model to learn accross them...
* They cannot forecast out of the scale of the training data.
* How can 2022's and 2023's autoregressive behaviour be used together to enhance the model, without having the level as a feature?

```{python}
from sktime.transformations.series.difference import Differencer
from sktime.transformations.series.boxcox import LogTransformer

global_forecaster3 = Differencer() * global_forecaster2
global_forecaster3.fit(y_train, X_train)
```

```{python}
y_pred_global3 = global_forecaster3.predict(fh=fh, X=X_test)
metric(y_true=y_test, y_pred=y_pred_global3, y_train=y_train)
```

```{python}
fig, ax = plt.subplots(figsize=(10, 4))
y_train.loc[0].plot(ax=ax, label="Train")
y_test.loc[0].plot(ax=ax, label="Test")
y_pred_global3.loc[0].plot(ax=ax, label="Global 4")
fig.show()
```

### Exogenous pipelines also for panel data!

```{python}
from sktime.transformations.series.fourier import FourierFeatures

fourier_features = FourierFeatures(sp_list=[365.25, 365.25/12], fourier_terms_list=[1, 1], freq="D")

global_forecaster4 = fourier_features ** global_forecaster3
global_forecaster4.fit(y_train, X_train)
```

```{python}
y_pred_global4 = global_forecaster4.predict(fh=fh, X=X_test)
metric(y_true=y_test, y_pred=y_pred_global4, y_train=y_train)
```

```{python}
metric(y_true=y_test, y_pred=y_pred_global4, y_train=y_train)
```

## 2.4. Probabilistic forecasting

When forecasting for retail, we often interested in the uncertainty of the forecasts.

* Safety stock
* Predict probability of stockouts

```{python}
from sktime.registry import all_estimators

all_estimators("forecaster", filter_tags={"capability:pred_int": True}, as_dataframe=True)
```

```{python}
from sktime.forecasting.auto_reg import AutoREG
from sktime.transformations.series.difference import Differencer
from sktime.transformations.series.fourier import FourierFeatures
from sktime.forecasting.conformal import ConformalIntervals

fourier_features = FourierFeatures(
    sp_list=[365.25, 365.25 / 12], fourier_terms_list=[1, 1], freq="D"
)
auto_reg = fourier_features ** (Differencer() * AutoREG())


conformal_forecaster = ConformalIntervals(
    forecaster=auto_reg, initial_window=365 * 2, sample_frac=0.5
)
```

```{python}
parallel_config = {
        "backend:parallel": "joblib",
        "backend:parallel:params": {"backend": "loky", "n_jobs": -1},
    }

conformal_forecaster.set_config(
    **parallel_config
)

conformal_forecaster.fit(y_train)
```

```{python}
y_pred_int = conformal_forecaster.predict_interval(fh=fh, coverage=0.9)
```

```{python}
y_pred_int
```

```{python}
plot_series(
    y_train.loc[10], y_test.loc[10], labels=["Train", "Test"], title="Panel data",
    pred_interval=y_pred_int.loc[10], markers=[None]*2
)
```

There are negative values in the data, which do not make sense for our problem.

We can use a model that predicts a distribution that does not allow negative values, such as the **negative binomial distribution**.

```{python}
from prophetverse import Prophetverse, PiecewiseLinearTrend, MAPInferenceEngine


prophet = Prophetverse(
    trend=PiecewiseLinearTrend(changepoint_interval=365),
    likelihood="negbinomial",
    inference_engine=MAPInferenceEngine()
)

prophet.set_config(
    **parallel_config
)

prophet.fit(y_train, X_train)
```

```{python}
y_pred_int_prophetverse = prophet.predict_interval(fh=fh, X=X_test, coverage=0.9)
```

```{python}
plot_series(
    y_train.loc[10], y_test.loc[10], labels=["Train", "Test"], title="Panel data",
    pred_interval=y_pred_int_prophetverse.loc[10], markers=[None]*2
)

plt.show()
```

### Example of metric for probabilistic forecasting

```{python}
from sktime.performance_metrics.forecasting.probabilistic import PinballLoss

pinball_loss = PinballLoss()

pd.DataFrame(
    {"Conformal": pinball_loss(y_true=y_test, y_pred=y_pred_int),
     "Prophetverse Negbinomial": pinball_loss(y_true=y_test, y_pred=y_pred_int_prophetverse)},
    index=["Pinball Loss"]
)
```

## 2.5. Deep learning models and zero-shot forecasting

* In addition to simple ML models, we can also use deep learning models for forecasting.
* There are some models with tailored architectures for time series forecasting.
* For example, N-BEATS is a deep learning model that can be used for forecasting.
  
* **Zero-shot forecasting** is extremely useful when a new product appears, a new warehouse... etc.
  
![](imgs/nbeats_simplified.png)

```{python}
from sktime.forecasting.pytorchforecasting import PytorchForecastingNBeats
from pytorch_forecasting.data.encoders import EncoderNormalizer

CONTEXT_LENGTH = 365
nbeats = PytorchForecastingNBeats(
    train_to_dataloader_params={"batch_size": 256},
    trainer_params={"max_epochs": 1},
    model_params={
        "stack_types": ["trend", "seasonality"], # One of the following values: “generic”, “seasonality” or “trend”.
        "num_blocks" : [2,2], # The number of blocks per stack. 
        "context_length": CONTEXT_LENGTH, # lookback period
        "expansion_coefficient_lengths" : [2, 5],
        "learning_rate": 1e-3,
    },
    dataset_params={

        "max_encoder_length": CONTEXT_LENGTH,
        "target_normalizer": EncoderNormalizer()
    },
)

nbeats.fit(y_train.astype(float), fh=fh)
```

```{python}
y_pred_nbeats = nbeats.predict(fh=fh, X=X_test)
```

```{python}
metric(y_true=y_test, y_pred=y_pred_nbeats, y_train=y_train)
```

```{python}
fig, ax = plt.subplots(figsize=(10, 4))
y_train.loc[10].plot(ax=ax, label="Train")
y_test.loc[10].plot(ax=ax, label="Test")
y_pred_nbeats.loc[10].plot(ax=ax, label="N-BEATS")
fig.show()
```

```{python}
new_y_train = (y_train.loc[0]**2 + y_train.loc[20]).astype(float)
new_y_test = (y_test.loc[0]**2 + y_test.loc[20]).astype(float)

# Plotting the new series
fig, ax = plt.subplots(figsize=(10, 4))
new_y_train["sales"].plot.line(ax=ax, label="New Train")
new_y_test["sales"].plot.line(ax=ax, label="New Test")
fig.show()
```

```{python}
y_pred_zeroshot = nbeats.predict(fh=fh, y=new_y_train)
```

```{python}
fig, ax = plt.subplots(figsize=(10, 4))
new_y_train["sales"].plot.line(ax=ax, label="New Train")
new_y_test["sales"].plot.line(ax=ax, label="New Test")
y_pred_zeroshot["sales"].plot.line(ax=ax, label="N-BEATS Zero-shot")
plt.legend()
plt.show()
```

